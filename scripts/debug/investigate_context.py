#!/usr/bin/env python3
"""
üîç INVESTIGACI√ìN NO INVASIVA - Contexto Espec√≠fico
Script para investigar qu√© contexto se est√° pasando al LLM sin modificar nada
"""

import asyncio
import os
import sys
from pathlib import Path

# A√±adir el directorio del proyecto al path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# Cargar variables de entorno desde archivo .env
from dotenv import load_dotenv
load_dotenv()

from app.services.rag_service import RAGService
from app.core.config import settings

async def investigate_context():
    """Investigar el contexto espec√≠fico que se pasa al LLM"""
    print("üîç INVESTIGACI√ìN NO INVASIVA - CONTEXTO ESPEC√çFICO")
    print("=" * 60)
    print("‚ö†Ô∏è SOLO LECTURA - NO SE MODIFICA NADA")
    print("=" * 60)
    
    try:
        # Inicializar RAG Service
        print("üîÑ Inicializando RAG Service...")
        rag_service = RAGService()
        print("‚úÖ RAG Service inicializado")
        
        # Preguntas problem√°ticas que sabemos que fallan
        problem_questions = [
            "¬øQu√© proyectos de IA has liderado?",
            "¬øCu√°l fue el logro m√°s significativo en AcuaMattic?",
            "¬øCu√°l es tu motivaci√≥n para un nuevo reto profesional?",
            "¬øTienes certificaci√≥n en AWS?",
        ]
        
        print(f"\nüß™ Investigando {len(problem_questions)} preguntas problem√°ticas...")
        
        for i, question in enumerate(problem_questions, 1):
            print(f"\n{'='*20} PREGUNTA {i}/{len(problem_questions)} {'='*20}")
            print(f"üìã Pregunta: '{question}'")
            
            # Obtener contexto relevante (igual que en el c√≥digo real)
            retriever = rag_service.vector_store.as_retriever(
                search_kwargs={"k": settings.VECTOR_SEARCH_K}
            )
            docs = retriever.get_relevant_documents(question)
            
            # Formatear contexto (igual que en el c√≥digo real)
            context = "\n\n".join([doc.page_content for doc in docs])
            
            print(f"üìö Chunks encontrados: {len(docs)}")
            print(f"üìù Longitud del contexto: {len(context)} caracteres")
            
            # Mostrar distribuci√≥n por source
            sources = {}
            for doc in docs:
                source = doc.metadata.get("source", "unknown")
                sources[source] = sources.get(source, 0) + 1
            print(f"üìä Distribuci√≥n por source: {sources}")
            
            # Mostrar los primeros chunks para entender el contenido
            print(f"\nüìÑ CONTENIDO DEL CONTEXTO:")
            print("-" * 40)
            
            for j, doc in enumerate(docs[:3]):  # Mostrar solo los primeros 3
                source = doc.metadata.get("source", "unknown")
                doc_id = doc.metadata.get("id", "unknown")
                content = doc.page_content
                
                print(f"\nChunk {j+1}:")
                print(f"  Source: {source}")
                print(f"  ID: {doc_id}")
                print(f"  Longitud: {len(content)} caracteres")
                print(f"  Preview: {content[:200]}...")
                print("-" * 30)
            
            # Mostrar el contexto completo si es corto
            if len(context) < 1000:
                print(f"\nüìÑ CONTEXTO COMPLETO:")
                print("-" * 40)
                print(context)
                print("-" * 40)
            else:
                print(f"\nüìÑ CONTEXTO COMPLETO (primeros 500 caracteres):")
                print("-" * 40)
                print(context[:500] + "...")
                print("-" * 40)
            
            print(f"\nüîç AN√ÅLISIS:")
            if len(docs) == 0:
                print("   ‚ùå PROBLEMA: No se encontraron chunks")
            elif len(context) < 100:
                print("   ‚ö†Ô∏è PROBLEMA: Contexto muy corto")
            else:
                print("   ‚úÖ Contexto disponible - El problema puede estar en el prompt")
            
            print("\n" + "="*80)
        
        print("\nüéØ CONCLUSIONES:")
        print("1. Si hay chunks disponibles pero el LLM usa fallback ‚Üí Problema en el prompt")
        print("2. Si no hay chunks disponibles ‚Üí Problema en la recuperaci√≥n")
        print("3. Si el contexto es muy corto ‚Üí Problema en el tama√±o de chunks")
        
        print("\n‚úÖ Investigaci√≥n completada - NO SE MODIFIC√ì NADA")
        
    except Exception as e:
        print(f"‚ùå Error en investigaci√≥n: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(investigate_context())
